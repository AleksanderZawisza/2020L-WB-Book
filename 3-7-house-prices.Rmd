## Which Neighbours Affected House Prices in the '90s? 

*Authors: Hubert Baniecki, Mateusz Polakowski (Warsaw University of Technology)*

### Abstract


The house price estimation task has a long-lasting history in economics and statistics. Nowadays, both worlds unite to exploit the machine learning approach and achieve the best results. In the literature, there are myriad of works discussing the performance-interpretability tradeoff apparent in the modelling of real estate values. In this paper, we propose a solution to this problem, which is a highly interpretable stacked model that outperforms the black-box models. We use it to examine neighbourhood parameters affecting the median house price of the United States regions in 1990.

### Introduction
  
Real estate value varies over numerous factors. These may be obvious like location or interior design, but also less apparent like the ethnicity and age of neighbours. Therefore, property price estimation is a demanding job that often requires a lot of experience and market knowledge. Is or was, because nowadays, Artificial Intelligence (AI) surpasses humans in this task [@realestate-ai]. Interested parties more often use tools like supervised Machine Learning (ML) models to precisely evaluate the property value and gain a competitive advantage [@realestate-ml1, @realestate-ml2, realestate-ml3].

The dilemma is in blindly trusting the prediction given by so-called black-box models. These are ML algorithms that take loads of various real estate data as input and return a house price estimation without giving their reasoning. Black-box complex nature is its biggest strength and weakness at the same time. This trait regularly entails high effectiveness but does not allow for interpretation of model outputs [@multiple-models]. Because of that, specialists interested in supporting their work with automated ML decision-making are more eager to use white-box models like linear regression or decision trees [@wb-vs-bb]. These do not achieve state-of-the-art performance efficiently, but instead, provide valuable information about the relationships present in data through model interpretation.

For many years houses have been popular properties; thus, they are of particular interest for ordinary people. What exact influence had the demographic characteristics of the house neighbourhood on its price in the '90s? Although in the absence of current technology, it has been hard to answer such question years ago [@history2001], now we can.

In this paper, we perform a case study on the actual United States Census data from 1990 [@housepricesdata] and deliver an interpretable white-box model that estimates the median house price by the region. We present multiple approaches to this problem and choose the best model, which achieves similar performance to complex black-boxes. Finally, using its interpretable nature, we answer various questions that give a new life to this historical data. 

### Related Work

The use of ML in the real estate domain is a well-documented ground [@realestate-ai] and not precisely a topic of this contribution. We relate to the works that aim to use Interpretable ML techniques [@christophmonlar] to interpret models predictions in the house price estimation problem. 
 
The state-of-the-art approach to house price estimation is to combine linear and semi-log regression models with the Hedonic Pricing Method [@hpm1, @hpm2], which aims to determine the extent that environmental or ecosystem factors affect the price of a good. [@hpm-iml] deliberately seeks to interpret the outcome and provide information about the parameters that affect property value. There are also comparisons between the linear white-box and black-box models [@wb-vs-bb, @multiple-models] which showcase the performance-interpretability tradeoff.

Nature of the topic might entail that the data is place-specific; therefore, part of the studies focus on a single location with the use of geospatial data. The case study on London [@localarea-network] links the street network community structure with house price, which takes into consideration the topology of the city. In contradiction, [@relative-absolute] uses the Oslo city data to explore the differences between the relative and absolute location attributes. Applying the data like the distance to the nearest shop and transportation is place-agnostic.

One of the new ideas is to utilize the location data from multiple sources in a Multi-Task Learning approach [@mlt]. It also studies the relationships between the tasks, which gives an extensive insight on prediction attributions. 

In this paper we partialy aim to enhance the use of regression decision tree models, which had been utilized to estimate the house prices based on their essential characteristics in [@houseprice-tree]. 

### Data

For this case study we use the *house_8L* dataset crafted from the data collected in 1990 by the United States Census Bureau. Each record stands for a distinct United States state while the target value is a median house price in a given region. The variables are presented in Table \@ref(tab:3-7-dataset).

```{r 3-7-dataset, echo = FALSE}
library(kableExtra)
text_tbl <- data.frame(
  "Original name" = c("price", "P3", "H15.1", "H5.2", "H40.4", "P11.3", "P16.2", "P19.2", "P6.4"),
  "New name" = c("price", "house_n", "avg_room_n", "forsale_h_pct", "forsale_6mplus_h_pct",
                 "age_25_64_pct", "family_2plus_h_pct", "black_h_pct", "asian_p_pct"),
  "Description" = c(
    "median price of the house in the region",
    "total number of households",
    "average number of rooms in an owner-occupied Housing Units", 
    "percentage of vacant Housing Units which are for sale only",
    "percentage of vacant-for-sale Housing Units vacant more then 6 months",
    "percentage of people between 25-64 years of age",
    "percentage of households with 2 or more persons which are family households",
    "percentage of households with black Householder",
    "percentage of people which are of Asian or Pacific Islander race"
  )
)
colnames(text_tbl) <- c("Original name", "New name", "Description")
kable(text_tbl, align=c('c', 'c', 'c'), caption="Description of variables present in the house_8L dataset.") %>%
  kable_styling("bordered", full_width = F) %>%
  column_spec(1, width = "10em", bold = F) %>%
  column_spec(2, width = "10em", bold = T) %>%
  column_spec(3, width = "40em", background = "white") 
```

Furthermore, we will apply our Metodology (Section 4) on a corresponding *house_16H* dataset, which has the same target but a different set of variables.
More correlated variables of a higher variance make it significantly harder to estimate the median house price in a given region.
Such validation will allow us to evaluate our model on a more demanding task.
The comprehensive description of used data can be found in [@housepricesdata].


### Methodology

In this section, we are going to focus on developing the best white-box model, which provides interpretability of features. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we focus on the residuals while the mean of absolute values of residuals is the easiest to interpret.

#### EDA

The main conclusions from the Exploratory Data Analysis are as follows:

1. The target value is very skewed (See Figure \@ref(fig:3-7-eda)).
2. There are 6 percentage and 2 count variables.
3. The dataset has over 22k data points.
4. There are 46 data points with unnaturally looking target value. 
5. There are no missing values.

```{r 3-7-eda, cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="(L) Histogram of the target values. (R) Examplary variable correlation with the target."}
knitr::include_graphics('images/3-7-eda.png')
```

Therefore we decided that:

1. We will not transform the skewed target because this might provide less interpretability.
2. There are not many possibilities for feature engineering.
3. We can reliably split the data into train and test using 2:1 ratio.
4. We suspect that the target value of 500001 is artificially made, so we remove these outliers.

Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we later focus on the residuals while the mean of absolute values of residuals is the easiest to interpret.

#### SAFE 

The first approach was using the SAFE [@gosiewska2019safe] technique to engineer new features and produce a linear regression model. We trained a well-performing black-box `ranger` [@ranger] model and extracted new interpretable features using its Partial Dependence Profiles [@pdp]. Then we used these features to craft a new linear model which indeed was better than the baseline linear model by about 10%. It is worth noting that both of these linear models had a hard time succeeding because of the target skewness.

#### Divide-and-conquer

In this section, we present the main contribution of this paper.
The divide-and-conquer idea has many computer science applications, e.g. in sorting algorithms, natural language processing, or parallel computing.
We decided to make use of its core principles in constructing the method for fitting the enhanced white-box model.
The final result is multiple tree models combined which decisions are easily interpretable.

The proposed algorithm presented in Figure \@ref(fig:3-7-algorithm) is:

1. Divide the target variable with `k` middle points into `k+1` groups.
2. Fit a black-box classifier on train data which predicts the belonging to the `i-th` group.
3. Use this classifier to divide the train and test data into `k+1` train and test subsets.
4. For every `i-th` subset fit a white-box estimator of target variable on the `i-th` train data.
5. Use the `i-th` estimator to predict the outcome of the `i-th` test data.

```{r 3-7-algorithm, cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="The divide-and-conquer algorithm used to construct an enhanced white-box model. (1) Divide the target variable with `k` middle points into `k+1` groups. (2) Fit a black-box classifier on train data which predicts the belonging to the `i-th` group. (3) Use this classifier to divide the train and test data into `k+1` train and test subsets. (4) For every `i-th` subset fit a white-box estimator."}
knitr::include_graphics('images/3-7-algorithm.png')
```

The final product is a stacked model with one classifier and `k+1` estimators. The exact models are for engineers to choose. It is worth noting that the unsupervised clustering method might be used instead of the classification model.

### Results

#### Final model

For the house price task, we chose `k = 1`, and the middle point was arbitrary chosen as `100k`, which divides the data into two groups in about a 10:1 ratio. We used the `ranger` random forest model as a black-box classifier and the `rpart` [@rpart] decision tree model as a white-box estimator. 

The `ranger` model had default parameters with `mtry=3`. The parameters of `rpart` models were: 

- `maxdepth = 4` - low depth reassures the interpretability of the model 
- `cp = 0.001` - lower complexity helps with the skewed target
- `minbucket = 1% of the training data` - more filled tree leaves adds up to higher interpretability 

Figure \@ref(fig:3-7-tree1) depicts the tree that estimates cheaper houses, while Figure \@ref(fig:3-7-tree2) presents the tree that estimates more expensive houses.

```{r 3-7-tree1, cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="The tree that estimates cheaper houses. Part of the stacked model."}
knitr::include_graphics('images/3-7-tree-cheap.svg')
```

```{r 3-7-tree2, cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="The tree that estimates more expensive houses. Part of the stacked model."}
knitr::include_graphics('images/3-7-tree-rich.svg')
```

Interpreting the model presented in Figures \@ref(fig:3-7-tree1) & \@ref(fig:3-7-tree2) leads to multiple conclusions. Firstly, we can observe the noticeable impact of features like the total number of households or the average number of rooms on the median price of the house in the region, which is compliant with basic intuitions. Then we see that the bigger percentage of people between 25-64 years of age the higher the prices. 

Finally, we come to the more critical features; the percentage of people which are of Asian or Pacific Islander race divides the prices in an opposing direction to the percentage of households with black Householder. These showcases which neighbours and how affected house prices in the `90s.
    
#### Comparison

We next compare our stacked model with baseline `ranger` and `rpart` models, respectively referred to as black-box and white-box. Our solution achieves competitive performance with interpretable features. 

**TODO: describe the plots**

```{r cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="Density of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses."}
knitr::include_graphics('images/3-7-density.png')
```

```{r cache=FALSE, out.width="800", fig.align="center", echo=FALSE, eval = is.html, fig.cap="Boxplots of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses."}
knitr::include_graphics('images/3-7-boxplot.png')
```

We present the comparison of MAE scores for all of the used models in this case study in Table \@ref(tab:3-7-table-results). There are two tasks with different variables, complexity and correlations. We calculate the scores on the test subsets. 

We can see that the linear models performed the worse, although the SAFE approach noticeably lowered the MAE. Then there is a decision tree which performed better but not so on the more laborious task. Both of the black-box models did a far better job at house price estimation. Finally, our stacked model is a champion with the best performance on both of the tasks.

```{r 3-7-table-results, echo = FALSE}
library(kableExtra)
text_tbl <- data.frame(
  'Model' = c('ranger', "xgboost", 'linear model', "SAFE on ranger", 'rpart', "stacked model"),
  'house_8L' = c(14829, 16035, 23057, 21408, 19195, 14605),
  'house_16H' = c(15602, 16499, 24051, 22601, 22145, 15273)
)
text_tbl <- text_tbl[c(3,4,5,2,1,6),]
text_tbl$house_8L <- paste0(signif(text_tbl$house_8L, 3)/1000, "k")
text_tbl$house_16H <- paste0(signif(text_tbl$house_16H, 3)/1000, "k")
rownames(text_tbl) <- NULL
kable(text_tbl, align=c('c', 'c', 'c'), caption="Comparison of the MAE score for all of the used models on test datasets. Green colour highlights white-box models, while the red colour is for black-box models.") %>%
  kable_styling("bordered", full_width = F) %>%
  column_spec(1, width = "20em", bold = T) %>%
  column_spec(2, width = "10em", bold = F) %>%
  column_spec(3, width = "10em", bold = F) %>%
  row_spec(c(4,5), background = "#f05a71", color = "black") %>%
  row_spec(c(1,2,3, 6), background = "#8bdcbe", color = "black") %>%
  add_header_above(c(" " = 1, "Dataset (test)" = 2)) 
  #row_spec(c(6), background = "#4378bf", color = "white")
```


### Conclusions

**TODO: Conclusions**

**TODO: add citations to bib without the conflicts**

@phdthesis{realestate-ai,
  author = {Conway, Jennifer},
  year = {2018},
  month = {01},
  url = {https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf},
  title = {{Artificial Intelligence and Machine Learning : Current Applications in Real Estate}}
}

@article{realestate-ml1,
  author = {Park, Byeonghwa and Bae, Jae},
  year = {2015},
  month = {04},
  title = {{Using machine learning algorithms for housing price prediction:
  The case of Fairfax County, Virginia housing data},
  volume = {42},
  journal = {Expert Systems with Applications},
  url = {https://doi.org/10.1016/j.eswa.2014.11.040}
}
@article{realestate-ml2,
  author = {Rafiei, Mohammad H. and Adeli, Hojjat},
  year = {2015},
  month = {08},
  pages = {04015066},
  title = {{A Novel Machine Learning Model for Estimation of Sale Prices of Real Estate Units}},
  volume = {142},
  journal = {Journal of Construction Engineering and Management},
  url = {https://doi.org/10.1061/(ASCE)CO.1943-7862.0001047}
}

@article{realestate-ml3,
  title={{Predicting housing price in China based on long short-term memory
  incorporating modified genetic algorithm}},
  author={Rui Liu and Lu Liu},
  journal={Soft Computing},
  year={2019},
  pages={1-10},
  url = {https://doi.org/10.1007/s00500-018-03739-w}
}

@article{wb-vs-bb,
  author = {Selim, Hasan},
  year = {2009},
  month = {03},
  pages = {2843-2852},
  title = {Determinants of house prices in Turkey: Hedonic regression
  versus artificial neural network},
  volume = {36},
  journal = {Expert Syst. Appl.},
  url = {https://doi.org/10.1016/j.eswa.2008.01.044}
}

@article{multiple-models,
  author = {Baldominos, Alejandro and Blanco, Iván and Moreno,
  Antonio and Iturrarte, Rubén and Bernárdez, Óscar and Afonso, Carlos},
  year = {2018},
  month = {11},
  pages = {2321},
  title = {Identifying Real Estate Opportunities Using Machine Learning},
  volume = {8},
  journal = {Applied Sciences},
  url = {https://doi.org/10.3390/app8112321}
}

@article{history2001,
  author = {Din, Allan and Hoesli, Martin and Bender, André},
  year = {2001},
  month = {02},
  title = {Environmental Variables and Real Estate Prices},
  volume = {38},
  journal = {Urban Studies},
  url = {https://doi.org/10.1080/00420980120080899}
}

@article{hpm-iml,
  author = {Özalp, Ayşe and Akinci, Halil},
  year = {2017},
  month = {12},
  title = {The use of hedonic pricing method to determine the parameters
  affecting residential real estate prices},
  volume = {10},
  journal = {Arabian Journal of Geosciences},
  url={https://doi.org/10.1007/s12517-017-3331-3}
}

@article{hpm1,
  title={Valuing goods' characteristics: an application of the hedonic price
  method to environmental attributes},
  author={Garrod, Guy D and Willis, Kenneth G},
  journal={Journal of Environmental management},
  volume={34},
  number={1},
  pages={59--76},
  year={1992},
  publisher={Academic Press}
}

@article{hpm2,
  author = {Randeniya, TD and Ranasinghe, Gayani and Amarawickrama, Susantha},
  year = {2017},
  month = {05},
  title = {A model to Estimate the Implicit Values of Housing Attributes
  by Applying the Hedonic Pricing Method},
  volume = {4},
  journal = {International Journal of Built Environment and Sustainability},
  url={https://doi.org/10.11113/ijbes.v4.n2.182}
}

@article{localarea-network,
  author = {Law, Stephen},
  year = {2017},
  month = {02},
  pages = {166-179},
  title = {Defining Street-based Local Area and measuring its effect
  on house price using a hedonic price approach: The case study of Metropolitan London},
  volume = {60},
  journal = {Cities},
  doi = {10.1016/j.cities.2016.08.008}
}

@article{relative-absolute,
  author = {Heyman, Axel and Sommervoll, Dag},
  year = {2019},
  month = {09},
  pages = {102373},
  title = {House prices and relative location},
  volume = {95},
  journal = {Cities},
  url={https://doi.org/10.1016/j.cities.2019.06.004}
}

@unknown{mlt,
  author = {Gao, Guangliang and Bao, Zhifeng and Cao, Jie and Qin, A. and Sellis,
  Timos and Fellow, and IEEE, and Wu, Zhiang},
  year = {2019},
  month = {01},
  url = {https://arxiv.org/abs/1901.01774},
  title = {Location-Centered House Price Prediction: A Multi-Task Learning Approach}
}

@article{houseprice-tree,
  author = {Fan, Gang-Zhi and Ong, Seow Eng and Koh, Hian},
  year = {2006},
  month = {11},
  pages = {2301-2316},
  title = {Determinants of House Price: A Decision Tree Approach},
  volume = {43},
  journal = {Urban Studies},
  doi = {10.1080/00420980600990928}
}